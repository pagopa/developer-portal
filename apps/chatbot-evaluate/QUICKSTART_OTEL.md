# OpenTelemetry Quick Start Guide

## ğŸ¯ What Was Done

Your AWS Lambda function is now fully instrumented with OpenTelemetry to identify heavy library calls and initialization times.

## ğŸ“Š Key Instrumentation Points

### Cold Start (Initialization)
The most critical area for Lambda performance:
- âœ… **Judge initialization** - Wrapped with `judge_initialization` span
- âœ… **LLM loading** - Tracked with `get_llm` span (includes Google GenAI import)
- âœ… **Embedding model loading** - Tracked with `get_embed_model` span
- âœ… **Evaluator setup** - Tracked with `create_evaluator` span (includes Ragas metrics)

### Request Processing
- âœ… **Lambda handler** - Overall execution time
- âœ… **Per-record processing** - Individual SQS message handling
- âœ… **LLM calls** - API calls to language models
- âœ… **Evaluation metrics** - Ragas evaluation execution
- âœ… **External services** - Langfuse score reporting

## ğŸš€ Quick Test

Run the test script to see traces in action:

```bash
cd /Users/walter.traspadini/src/pagopa/developer-portal/apps/chatbot-evaluate
python3 test_otel.py
```

This will output trace spans to the console, showing you exactly where time is spent.

## ğŸ”§ Configuration

### For Local Testing
```bash
export OTEL_TRACES_EXPORTER=console
export OTEL_SERVICE_NAME=chatbot-evaluate
```

### For AWS Lambda (CloudWatch Logs)
Add these environment variables to your Lambda function:
```
OTEL_TRACES_EXPORTER=console
OTEL_SERVICE_NAME=chatbot-evaluate-prod
```

Traces will appear in CloudWatch Logs as JSON.

### For AWS X-Ray Integration
1. Add the ADOT Lambda layer (ARN varies by region):
   ```
   arn:aws:lambda:eu-south-1:184161586896:layer:aws-otel-python-amd64-ver-1-26-0:1
   ```

2. Set environment variables:
   ```
   OTEL_TRACES_EXPORTER=otlp
   OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317
   AWS_LAMBDA_EXEC_WRAPPER=/opt/otel-instrument
   ```

## ğŸ“ˆ What to Look For

### High Cold Start Times
If your Lambda cold starts are slow, check these spans:

1. **`import_google_genai`** - Google AI library import (can be 1-5 seconds)
2. **`import_google_genai_embedding`** - Embedding library import
3. **`instantiate_google_llm`** - Creating LLM instance
4. **`instantiate_google_embedding`** - Creating embedding instance
5. **`init_metrics`** - Initializing Ragas metrics (Faithfulness, Relevancy, Precision)

### High Request Processing Times
If requests are slow, check:

1. **`llm_acomplete`** - LLM API calls (depends on Google API latency)
2. **`ragas_evaluate`** - Evaluation can make multiple LLM calls
3. **`add_langfuse_scores`** - Network call to Langfuse service

## ğŸ¨ Trace Hierarchy

```
judge_initialization (Cold Start)
â”œâ”€â”€ judge_init
â”‚   â”œâ”€â”€ load_settings
â”‚   â”œâ”€â”€ get_llm
â”‚   â”‚   â”œâ”€â”€ import_google_genai
â”‚   â”‚   â””â”€â”€ instantiate_google_llm
â”‚   â””â”€â”€ create_evaluator
â”‚       â”œâ”€â”€ get_embed_model
â”‚       â”‚   â”œâ”€â”€ import_google_genai_embedding
â”‚       â”‚   â””â”€â”€ instantiate_google_embedding
â”‚       â””â”€â”€ instantiate_evaluator
â”‚           â”œâ”€â”€ wrap_llm
â”‚           â”œâ”€â”€ wrap_embedder
â”‚           â””â”€â”€ init_metrics
â”‚               â”œâ”€â”€ init_response_relevancy
â”‚               â”œâ”€â”€ init_context_precision
â”‚               â””â”€â”€ init_faithfulness

lambda_handler (Request Processing)
â”œâ”€â”€ process_record_0
â”‚   â””â”€â”€ judge_evaluate
â”‚       â””â”€â”€ judge_evaluate_method
â”‚           â”œâ”€â”€ condense_query (if messages exist)
â”‚           â”‚   â”œâ”€â”€ messages_to_chathistory
â”‚           â”‚   â””â”€â”€ llm_acomplete
â”‚           â”œâ”€â”€ evaluator_evaluate
â”‚           â”‚   â””â”€â”€ evaluator_evaluate_method
â”‚           â”‚       â”œâ”€â”€ create_sample
â”‚           â”‚       â”œâ”€â”€ ragas_evaluate
â”‚           â”‚       â””â”€â”€ process_scores
â”‚           â””â”€â”€ add_langfuse_scores
â””â”€â”€ process_record_1
    â””â”€â”€ ...
```

## ğŸ’¡ Optimization Tips

Based on the traces, you might want to:

1. **Use Lambda Provisioned Concurrency** if cold starts are too slow
2. **Move heavy imports to lazy loading** if specific imports are slow
3. **Cache model instances** if you're reinitializing unnecessarily
4. **Use Lambda Layers** for large dependencies (Google AI libraries)
5. **Increase Lambda memory** - more memory = faster CPU for initialization

## ğŸ“š More Details

See [OTEL_INSTRUMENTATION.md](./OTEL_INSTRUMENTATION.md) for comprehensive documentation.

## âœ… Changes Made

Files modified:
- âœï¸ `src/lambda_function.py` - Added OpenTelemetry initialization and handler tracing
- âœï¸ `src/modules/judge.py` - Added tracing to Judge class and evaluation
- âœï¸ `src/modules/models.py` - Added tracing to model loading functions
- âœï¸ `src/modules/evaluator.py` - Added tracing to Evaluator class

Files created:
- âœ¨ `src/otel_config.py` - OpenTelemetry configuration
- âœ¨ `OTEL_INSTRUMENTATION.md` - Comprehensive documentation
- âœ¨ `test_otel.py` - Test script to verify instrumentation

Dependencies already installed in `pyproject.toml`:
- âœ… `opentelemetry-api`
- âœ… `opentelemetry-sdk`
- âœ… `opentelemetry-exporter-otlp`
- âœ… `opentelemetry-distro`
- âœ… `opentelemetry-instrumentation-aws-lambda`
- âœ… `opentelemetry-instrumentation-botocore`

## ğŸ¯ Next Steps

1. Test locally with `python3 test_otel.py`
2. Deploy to Lambda with environment variables set
3. Analyze traces in CloudWatch Logs or X-Ray
4. Identify bottlenecks and optimize accordingly
